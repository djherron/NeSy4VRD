# NeSy4VRD: Distributed Annotation Enhancement

A valuable feature of the **NeSy4VRD extensibility support infrastructure** is that it enables a new **model of collaboration** for **dataset creation and enhancement**. This model embraces and combines the concepts of *continuous improvement* and of *distributed, independent, incremental enhancement*.  The name we have adopted for this new dataset creation and enhancement collaboration model is **Distributed Annotation Enhancement** (**DAE**).

## Distributed Annotation Enhancement: overview

The **NeSy4VRD extensibility support infrastructure** enables the **DAE** model of collaboration by virtue of the fact that it facilitates all of the following activites:
1. **undertaking** NeSy4VRD annotation customisation projects;
2. **sharing** NeSy4VRD annotation customisation projects;
3. **reusing** individual NeSy4VRD annotation customisation projects undertaken and shared by other researchers;
4. **composing** multiple NeSy4VRD annotation customisation projects shared by other researchers in order to create unique, enriched datasets that combine the enhancements of diverse customisation projects. 

A researcher might typically initially **undertake** a NeSy4VRD annotation customisation project for personal and private reasons: e.g. to advance their particular research interests. But having **undertaken** such a project, a researcher might come to feel that their annotation customisations may have value for others and opt to **share** the project. This gives other researchers the option to **reuse** the shared project, by itself, in their own research.  Further, it also gives other researchers the option to **reuse** the annotation customisation project simultaneously with other shared projects, by **composing** (or combining) several of them to create a unique, consolidated set of NeSy4VRD visual relationship annotations that aggregates the annotation enhancements of multiple, independent customisation projects.

The details around **undertaking** NeSy4VRD visual relationship annotation customisation projects using the **NeSy4VRD extensibility support infrastructure** are documented extensively throughout this `extensibility` folder and its `analysis`, `protocol`, and `workflow` subfolders.

For details of the **Distributed Annotation Enhancement** model of collaboration concerning **sharing**, **reusing** and **composing** NeSy4VRD annotation customisation projects, please see the following companion README files here in the `extensibility` folder:
* README_SharingCustomisationProjects.md
* README_UsingSharedCustomisationProjects.md

## Distributed Annotation Enhancement vs. Crowdsourcing

The conventional model for collaborative data annotation is the well-known **crowdsourcing** (**CS**) model: **outsourcing** the annotation task to a **crowd** of dispersed contributors. The **Distributed Annotation Enhancement** (**DAE**) model for collaborative data annotation (enabled by the **NeSy4VRD extensibility support infrastructure**) is related but differs from the **CS** model in many respects. Here we highlight the several ways that these two models for collaborative data annotation differ from one another.

#### project nature: one large vs multiple small
* CS: one large project (annotating an entire dataset) is partitioned into multiple smaller subprojects
* DAE: multiple, small projects (enhancing the annotations of a dataset) are undertaken separately and independently

#### project organisation: centralised vs distributed
* CS: some central authority exists that organises and manages the master project and aggregates the results of the multiple subprojects into the final annotated dataset
* DAE: no central authority exists because there is no overarching master project; all annotation enhancement projects are fully distributed, uncoordinated and undertaken for private research reasons; if shared, the results of a project may be reused in isolation by other researchers or, optionally, multiple shared projects may be composed (assembled) into arbitrary permutations and combinations based on the particular interests and choices of particular AI researchers

#### project contributors:
* CS: anonymous individuals with diverse backgrounds and interests
* DAE: anonymous AI researchers pursuing diverse but related AI research interests

#### project contributor motivations:
* CS: monetary compensation, typically; sometimes voluntary
* DAE: pursuing a personal AI research vision 

#### time dimension:
* CS: the one large data annotation project is a *point in time* project; it may take some time to complete, but it has a defined start time and end time
* DAE: distributed annotation enhancement is more *longitudinal* in nature; it's an *ongoing* series of disjoint, uncoordinated projects undertaken *over time*, driven by the particular research interests of particular AI researchers

#### annotation quality:
* CS: the CS data annotation model is known to suffer from systemic quality issues and to often deliver annotation results that are replete with errors and highly unsatisfactory in many respects
* DAE: the DAE model promises delivery of top quality annotation enhancements; each project is presumed to be curated by an individual AI researcher having both the motivation and the understanding of the task necessary to ensure consistency and delivery of high quality work




